# -*- coding: utf-8 -*-
"""python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ajHzm15O2VgeRSgJw5kiQYht9zW9QrfK
"""

# ------------------------------
# Multimodal fusion pipeline for your uploaded files
# Expects (in working dir or uploaded):
#   F_audio_full.npy, Y_audio_full.npy
#   F_facial_full.npy, Y_facial_full.npy
#   heart_rate_emotion_dataset.csv  (columns: HeartRate, Emotion)
# ------------------------------

# Install dependencies (run once in Colab)
!pip install -q librosa imbalanced-learn scikit-learn joblib numpy pandas tqdm xgboost

# Imports
import os, sys
import numpy as np
import pandas as pd
from collections import Counter
from tqdm import tqdm
import joblib, warnings
warnings.filterwarnings('ignore')
SEED = 42
np.random.seed(SEED)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
from imblearn.over_sampling import SMOTE

print("Libraries loaded.")

# ---------- Helper: safe load ----------
def safe_load(path):
    return np.load(path, allow_pickle=True) if os.path.exists(path) else None

# ---------- 1) Load your provided files (try /mnt/data and working dir) ----------
candidates = [
    '/mnt/data/F_audio_full.npy', 'F_audio_full.npy', '/content/F_audio_full.npy'
]
F_audio = None
for p in candidates:
    if os.path.exists(p):
        F_audio = np.load(p, allow_pickle=True)
        print("Loaded audio features from", p)
        break

candidates = ['/mnt/data/Y_audio_full.npy', 'Y_audio_full.npy', '/content/Y_audio_full.npy']
Y_audio = None
for p in candidates:
    if os.path.exists(p):
        Y_audio = np.load(p, allow_pickle=True)
        print("Loaded Y_audio from", p)
        break

candidates = ['/mnt/data/F_facial_full.npy', 'F_facial_full.npy', '/content/F_facial_full.npy']
F_face = None
for p in candidates:
    if os.path.exists(p):
        F_face = np.load(p, allow_pickle=True)
        print("Loaded face features from", p)
        break

candidates = ['/mnt/data/Y_facial_full.npy', 'Y_facial_full.npy', '/content/Y_facial_full.npy']
Y_face = None
for p in candidates:
    if os.path.exists(p):
        Y_face = np.load(p, allow_pickle=True)
        print("Loaded Y_face from", p)
        break

# Load physiological CSV (heart_rate_emotion_dataset.csv)
csv_paths = ['/mnt/data/heart_rate_emotion_dataset.csv', 'heart_rate_emotion_dataset.csv', '/content/heart_rate_emotion_dataset.csv']
df_phys = None
for p in csv_paths:
    if os.path.exists(p):
        df_phys = pd.read_csv(p)
        print("Loaded physio CSV from", p)
        break

# Quick sanity prints
print("\nShapes (or None):")
print("F_audio:", None if F_audio is None else F_audio.shape)
print("Y_audio:", None if Y_audio is None else Y_audio.shape)
print("F_face:", None if F_face is None else F_face.shape)
print("Y_face:", None if Y_face is None else Y_face.shape)
print("Physio CSV:", None if df_phys is None else df_phys.shape)

# ---------- 2) Build Physio features (HeartRate -> small engineered vector) ----------
F_phys = None
Y_phys = None
phys_ids = None
if df_phys is not None:
    # Expect column `HeartRate` and `Emotion` (as your uploaded CSV had)
    # If there's a header use that; otherwise adapt.
    # Build feature vector per row: [mean, std, min, max, median, rms] of available numeric cols (or of HeartRate)
    numeric_cols = df_phys.select_dtypes(include=[np.number]).columns.tolist()
    if len(numeric_cols) == 0 and 'HeartRate' in df_phys.columns:
        numeric_cols = ['HeartRate']
    feats = []
    labs = []
    for _, row in df_phys.iterrows():
        vals = []
        for c in numeric_cols:
            try:
                v = float(row[c])
                vals.append(v)
            except:
                pass
        if len(vals) == 0:
            continue
        arr = np.array(vals)
        feat = np.array([
            arr.mean(),
            arr.std() if arr.size>1 else 0.0,
            arr.min(),
            arr.max(),
            np.median(arr),
            np.sqrt(np.mean(arr**2))  # rms
        ])
        feats.append(feat)
        # Try to fetch emotion label from a column named Emotion or similar
        lab = None
        for colname in ['Emotion','emotion','Label','label','emotion_label','emo']:
            if colname in df_phys.columns:
                lab = row[colname]
                break
        labs.append(lab if lab is not None else 'unknown')
    if len(feats) > 0:
        F_phys = np.vstack(feats)
        Y_phys = np.array(labs)
        print("Constructed physiological features:", F_phys.shape, "labels:", Y_phys.shape)
    else:
        print("No physio features could be constructed from CSV.")

# ---------- 3) Basic augmentation functions per modality ----------
import scipy.stats as st
def augment_audio(X):
    X = np.asarray(X)
    if X.ndim == 1: X = X.reshape(1,-1)
    means = X.mean(axis=1).reshape(-1,1)
    stds = X.std(axis=1).reshape(-1,1)
    mins = X.min(axis=1).reshape(-1,1)
    maxs = X.max(axis=1).reshape(-1,1)
    skew = st.skew(X, axis=1).reshape(-1,1)
    kurt = st.kurtosis(X, axis=1).reshape(-1,1)
    return np.hstack([X, means, stds, mins, maxs, skew, kurt])

def augment_face(X):
    X = np.asarray(X)
    if X.ndim == 1: X = X.reshape(1,-1)
    means = X.mean(axis=1).reshape(-1,1)
    stds = X.std(axis=1).reshape(-1,1)
    p25 = np.percentile(X, 25, axis=1).reshape(-1,1)
    p75 = np.percentile(X, 75, axis=1).reshape(-1,1)
    return np.hstack([X, means, stds, p25, p75])

def augment_phys(X):
    X = np.asarray(X)
    if X.ndim == 1: X = X.reshape(1,-1)
    means = X.mean(axis=1).reshape(-1,1)
    stds = X.std(axis=1).reshape(-1,1)
    mins = X.min(axis=1).reshape(-1,1)
    maxs = X.max(axis=1).reshape(-1,1)
    median = np.median(X, axis=1).reshape(-1,1)
    return np.hstack([X, means, stds, mins, maxs, median])

# Apply augmentation if modality exists
F_audio_aug = augment_audio(F_audio) if F_audio is not None else None
F_face_aug  = augment_face(F_face)   if F_face is not None else None
F_phys_aug  = augment_phys(F_phys)   if F_phys is not None else None

print("\nAugmented shapes:")
print("F_audio_aug:", None if F_audio_aug is None else F_audio_aug.shape)
print("F_face_aug :", None if F_face_aug is None else F_face_aug.shape)
print("F_phys_aug :", None if F_phys_aug is None else F_phys_aug.shape)

# ---------- 4) Build lists for alignment and alignment strategy ----------
# Preferred: align by provided label arrays (Y_audio, Y_face, Y_phys)
# If those are present and matching -> horizontal concat truncated to min length
Xs = []
Ys = []
names = []
if F_audio_aug is not None and Y_audio is not None:
    Xs.append(F_audio_aug); Ys.append(np.asarray(Y_audio).reshape(-1)); names.append('audio')
if F_face_aug is not None and Y_face is not None:
    Xs.append(F_face_aug); Ys.append(np.asarray(Y_face).reshape(-1)); names.append('face')
if F_phys_aug is not None and Y_phys is not None:
    Xs.append(F_phys_aug); Ys.append(np.asarray(Y_phys).reshape(-1)); names.append('phys')

if len(Xs) == 0:
    raise ValueError("No modalities with both features and labels available. Please ensure at least one modality has features and labels.")

# Check if all Ys identical elementwise
same = all(np.array_equal(Ys[0], y) for y in Ys[1:])
if same:
    min_n = min(x.shape[0] for x in Xs)
    Xs_trunc = [x[:min_n] for x in Xs]
    Y_common = Ys[0][:min_n]
    X_concat = np.hstack([x if x.ndim==2 else x.reshape(-1,1) for x in Xs_trunc])
    print("\nAligned by identical label arrays. Concatenated shape:", X_concat.shape)
else:
    # Fallback: align by label-sequence greedy matching (intersection by sequence)
    print("\nLabel arrays differ. Attempting greedy label-sequence alignment...")
    base = Ys[0]
    indices = []
    for y in Ys:
        idxs = []
        y_arr = np.asarray(y).reshape(-1)
        b_idx = 0
        for val in base:
            found = np.where(y_arr[b_idx:]==val)[0]
            if len(found)==0:
                idxs.append(-1)
            else:
                idxs.append(b_idx + found[0])
                b_idx = b_idx + found[0] + 1
        indices.append(idxs)
    mask = np.ones(len(base), dtype=bool)
    for j in range(1, len(indices)):
        mask &= (np.array(indices[j])!=-1)
    sel_idx_base = np.where(mask)[0]
    if len(sel_idx_base) == 0:
        raise ValueError("Could not align label sequences across modalities. Provide matching label arrays (Y_*.npy) or a shared Y.npy.")
    # build selected arrays
    Xs_sel = []
    for k,x in enumerate(Xs):
        if k==0:
            Xs_sel.append(x[sel_idx_base])
        else:
            idxs = np.array(indices[k])[sel_idx_base]
            Xs_sel.append(x[idxs])
    Y_common = base[sel_idx_base]
    X_concat = np.hstack([x if x.ndim==2 else x.reshape(-1,1) for x in Xs_sel])
    print("Aligned by sequence heuristic. Concatenated shape:", X_concat.shape)

# Filter only recognized emotions
COMMON_EMOTIONS = ['angry','sad','fear','happy','neutral','disgust','surprise']
mask_valid = np.array([str(y).lower() in COMMON_EMOTIONS for y in Y_common])
X_concat = X_concat[mask_valid]
Y_concat = np.array([str(y).lower() for y in Y_common])[mask_valid]
print("\nAfter filtering to recognized emotions -> samples:", X_concat.shape[0])
print("Class counts:", Counter(Y_concat))

if X_concat.shape[0] == 0:
    raise ValueError("No labelled samples after filtering. Check your label arrays.")

# ---------- 5) Per-modality selection: compute column splits to know where each modality lives
col_splits = []
start = 0
if F_audio_aug is not None:
    d = F_audio_aug.shape[1]; col_splits.append(('audio', start, start+d)); start += d
if F_face_aug is not None:
    d = F_face_aug.shape[1]; col_splits.append(('face', start, start+d)); start += d
if F_phys_aug is not None:
    d = F_phys_aug.shape[1]; col_splits.append(('phys', start, start+d)); start += d

print("Per-modality column splits:", col_splits)

# ---------- 6) Per-modality scaling/PCA/SelectKBest and concatenation ----------
selected_parts = []
per_mod_scalers = {}
per_mod_selectors = {}
per_mod_pcas = {} # Dictionary to store PCA objects
for (name,s,e) in col_splits:
    Xm = X_concat[:, s:e]
    scaler = StandardScaler(); Xm_s = scaler.fit_transform(Xm)
    per_mod_scalers[name] = scaler # Store scaler
    # PCA when wide
    if Xm_s.shape[1] > 200:
        pca = PCA(n_components=0.95, svd_solver='full')
        Xm_s = pca.fit_transform(Xm_s)
        per_mod_pcas[name] = pca # Store PCA object
        print(f"{name}: applied PCA -> {Xm_s.shape[1]} dims")
    selector = SelectKBest(mutual_info_classif, k=min(150, Xm_s.shape[1]))
    Xm_k = selector.fit_transform(Xm_s, Y_concat)
    selected_parts.append(Xm_k)
    per_mod_selectors[name] = selector # Store selector
    print(f"{name}: {Xm.shape[1]} -> {Xm_k.shape[1]} selected")

X_selected = np.hstack(selected_parts)
print("Concatenated selected feature shape:", X_selected.shape)

# ---------- 7) Balance with SMOTE and train classifier ----------
print("Before SMOTE:", Counter(Y_concat))
sm = SMOTE(random_state=SEED)
X_bal, y_bal = sm.fit_resample(X_selected, Y_concat)
print("After SMOTE:", Counter(y_bal))

clf_rf = RandomForestClassifier(n_estimators=400, random_state=SEED, n_jobs=-1)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
accs, f1s = [], []
print("\nTraining RandomForest (5-fold CV)...")
for fold,(tr,te) in enumerate(skf.split(X_bal, y_bal),1):
    clf_rf.fit(X_bal[tr], y_bal[tr])
    p = clf_rf.predict(X_bal[te])
    a = accuracy_score(y_bal[te], p)
    f = f1_score(y_bal[te], p, average='weighted')
    accs.append(a); f1s.append(f)
    print(f"Fold {fold}: acc={a:.4f}, f1={f:.4f}")

print("\nRandomForest mean acc:", np.mean(accs), "mean f1:", np.mean(f1s))

# Optional: XGBoost if installed
try:
    import xgboost as xgb
    clf_xgb = xgb.XGBClassifier(n_estimators=300, use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=4)
    accs_x, f1s_x = [], []
    print("\nTraining XGBoost (5-fold CV)...")
    for tr,te in skf.split(X_bal, y_bal):
        clf_xgb.fit(X_bal[tr], y_bal[tr])
        p = clf_xgb.predict(X_bal[te])
        accs_x.append(accuracy_score(y_bal[te], p))
        f1s_x.append(f1_score(y_bal[te], p, average='weighted'))
    print("XGB mean acc:", np.mean(accs_x), "mean f1:", np.mean(f1s_x))
except Exception as e:
    clf_xgb = None
    print("XGBoost unavailable or failed:", e)

# ---------- 8) Holdout evaluation ----------
X_train, X_test, y_train, y_test = train_test_split(X_bal, y_bal, test_size=0.15, stratify=y_bal, random_state=SEED)
clf_rf.fit(X_train, y_train)
y_pred = clf_rf.predict(X_test)
print("\nHoldout results (RandomForest):")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 weighted:", f1_score(y_test, y_pred, average='weighted'))
print(classification_report(y_test, y_pred))

# ---------- 9) Save artifact ----------
artifact = {
    'model_rf': clf_rf,
    'model_xgb': clf_xgb,
    'per_mod_scalers': per_mod_scalers,
    'per_mod_selectors': per_mod_selectors,
    'per_mod_pcas': per_mod_pcas, # Save PCA objects
    'col_splits': col_splits
}
joblib.dump(artifact, 'multimodal_emotion_artifact.joblib')
print("\nSaved multimodal_emotion_artifact.joblib in working directory.")